{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os.path as osp\nimport torch.utils.data as data\nfrom torchvision import models, transforms\nimport torch.optim as optim\nfrom torchvision import models\nfrom tqdm import tqdm\n\nimport numpy as np\nimport random","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_imgs))\nprint(len(test_imgs))","execution_count":6,"outputs":[{"output_type":"stream","text":"30083\n3582\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_annot = pd.read_csv(train_annot_path)\n\n\n\ntrain_ett_annot = train_annot.iloc[:,1:12]\n\nprint(train_ett_annot)","execution_count":32,"outputs":[{"output_type":"stream","text":"       ETT - Abnormal  ETT - Borderline  ETT - Normal  NGT - Abnormal  \\\n0                   0                 0             0               0   \n1                   0                 0             1               0   \n2                   0                 0             0               0   \n3                   0                 0             0               0   \n4                   0                 0             0               0   \n...               ...               ...           ...             ...   \n30078               0                 0             1               0   \n30079               0                 0             0               0   \n30080               0                 0             1               0   \n30081               0                 0             0               0   \n30082               0                 0             1               0   \n\n       NGT - Borderline  NGT - Incompletely Imaged  NGT - Normal  \\\n0                     0                          0             1   \n1                     0                          1             0   \n2                     0                          0             0   \n3                     0                          0             0   \n4                     0                          0             0   \n...                 ...                        ...           ...   \n30078                 0                          0             0   \n30079                 0                          0             0   \n30080                 0                          1             0   \n30081                 0                          0             0   \n30082                 0                          0             0   \n\n       CVC - Abnormal  CVC - Borderline  CVC - Normal  \\\n0                   0                 0             0   \n1                   0                 0             1   \n2                   0                 1             0   \n3                   1                 0             0   \n4                   0                 0             1   \n...               ...               ...           ...   \n30078               0                 1             1   \n30079               0                 0             1   \n30080               1                 0             1   \n30081               0                 1             0   \n30082               0                 0             1   \n\n       Swan Ganz Catheter Present  \n0                               0  \n1                               0  \n2                               0  \n3                               0  \n4                               0  \n...                           ...  \n30078                           0  \n30079                           0  \n30080                           0  \n30081                           0  \n30082                           0  \n\n[30083 rows x 11 columns]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 前処理クラス ImageTransform\n* 画像データをTensor型に変換\n* データ拡張"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageTransform():\n    \"\"\"\n    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n    画像のサイズをリサイズし、色を標準化する。\n    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n\n\n    Attributes\n    ----------\n    resize : int\n        リサイズ先の画像の大きさ。\n    mean : (R, G, B)\n        各色チャネルの平均値。\n    std : (R, G, B)\n        各色チャネルの標準偏差。\n    \"\"\"\n\n    def __init__(self, resize, mean, std):\n        self.data_transform = {\n            'train': transforms.Compose([\n                transforms.RandomResizedCrop(\n                    resize, scale=(0.5, 1.0)),  # データオーギュメンテーション\n                transforms.RandomHorizontalFlip(),  # データオーギュメンテーション\n                transforms.ToTensor(),  # テンソルに変換\n                transforms.Normalize(mean, std)  # 標準化\n            ]),\n            'val': transforms.Compose([\n                transforms.Resize(resize),  # リサイズ\n                transforms.CenterCrop(resize),  # 画像中央をresize×resizeで切り取り\n                transforms.ToTensor(),  # テンソルに変換\n                transforms.Normalize(mean, std)  # 標準化\n            ])\n        }\n\n    def __call__(self, img, phase='train'):\n        \"\"\"\n        Parameters\n        ----------\n        phase : 'train' or 'val'\n            前処理のモードを指定。\n        \"\"\"\n        return self.data_transform[phase](img)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_datapath_list(phase=\"train\"):\n    \"\"\"\n    データのパスを格納したリストを作成する。\n\n    Parameters\n    ----------\n    phase : 'train' or 'val'\n        訓練データか検証データかを指定する\n\n    Returns\n    -------\n    path_list : list\n        データへのパスを格納したリスト\n    \"\"\"\n\n    rootpath = \"/kaggle/input/ranzcr-clip-catheter-line-classification\"\n    if phase == \"train\":\n        target_path = osp.join(rootpath,phase+'/*.jpg')\n    else:\n        target_path = osp.join(rootpath,phase+'/*.jpg')\n    \n    print(target_path)\n\n    path_list = []  # ここに格納する\n\n    # globを利用してサブディレクトリまでファイルパスを取得する\n    for path in glob.glob(target_path):\n        path_list.append(path)\n    \n    num_train = len(path_list)*0.9\n    \n    train_path_list = []\n    val_path_list = []\n    \n    if phase == \"train\":\n        return path_list[:int(num_train)], path_list[int(num_train):]\n    else:\n        return path_list\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RanzcrDataset(data.Dataset):\n    \"\"\"\n    Datasetクラス。PyTorchのDatasetクラスを継承。\n\n    Attributes\n    ----------\n    file_list : リスト\n        画像のパスを格納したリスト\n    transform : object\n        前処理クラスのインスタンス\n    phase : 'train' or 'test'\n        学習か訓練かを設定する。\n    \"\"\"\n\n    def __init__(self, file_list, annotation_path,transform=None, phase='train'):\n        self.file_list = file_list  # ファイルパスのリスト\n        self.transform = transform  # 前処理クラスのインスタンス\n        self.phase = phase  # train or valの指定\n        self.annotation_path = annotation_path ## アノテーションデータ\n\n    def __len__(self):\n        '''画像の枚数を返す'''\n        return len(self.file_list)\n\n    def __getitem__(self, index):\n        '''\n        前処理をした画像のTensor形式のデータとラベルを取得\n        '''\n\n        # index番目の画像をロード\n        img_path = self.file_list[index]\n        img = Image.open(img_path)  # [高さ][幅][色RGB]\n        img = img.convert(\"RGB\")\n\n        # 画像の前処理を実施\n        img_transformed = self.transform(\n            img, self.phase)  # torch.Size([3, 224, 224])\n        \n        annot_data = pd.read_csv(self.annotation_path)\n        \n        img_name = img_path.split(\"/\")[-1].strip(\".jpg\")\n        #print(img_name)\n        \n        annot = annot_data[annot_data[\"StudyInstanceUID\"]==img_name]\n        #print(\"img_name;{}¥tannot:{}\".format(img_name, annot.iloc[:,1:4]))\n        label = annot.iloc[:,1:12].values\n        label = label[0].reshape(-1)\n        #print(np.argmax(label))\n        #lable = torch.from_numpy(label.astype(np.float32)).clone()\n        #print(label.shape)\n        #print(label)\n        #label = torch.from_numpy(label.astype(np.uint8)).clone()\n        #label = torch.unsqueeze(label,0)\n        \n        return img_transformed, label\n        #return img_transformed, np.argmax(label)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 乱数のシードを設定\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 訓練、テストデータのアノテーションパス\ntrain_annot_path = \"/kaggle/input/ranzcr-clip-catheter-line-classification/train.csv\"\n\n# 訓練, テストの画像へのファイルパスのリストを作成する\ntrain_list,val_list = make_datapath_list(phase=\"train\")\n#val_list = make_datapath_list(phase=\"val\")\n\nprint(len(train_list), len(val_list))\n\n# Datasetを作成する\nsize = 224\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ntrain_dataset = RanzcrDataset(\n    file_list=train_list, annotation_path=train_annot_path,\n    transform=ImageTransform(size, mean, std), phase='train')\n\nval_dataset = RanzcrDataset(\n    file_list=val_list, annotation_path=train_annot_path,\n    transform=ImageTransform(size, mean, std), phase='val')\n\n# DataLoaderを作成する\nbatch_size = 32\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False)\n\n\n# 辞書オブジェクトにまとめる\ndataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}","execution_count":21,"outputs":[{"output_type":"stream","text":"/kaggle/input/ranzcr-clip-catheter-line-classification/train/*.jpg\n24066 6017\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習済みのResNet50 モデルをロード\n\n# resnet50モデルのインスタンスを生成\nuse_pretrained = True  # 学習済みのパラメータを使用\nnet = models.resnet50(pretrained=use_pretrained)\n\n# resnetの最後の出力層の出力ユニットをアリとハチの2つに付け替える\nin_features = net.fc.in_features\nnet.fc = nn.Linear(in_features=in_features, out_features=11)\n\n# 訓練モードに設定\nnet.train()\n\nprint('ネットワーク設定完了：学習済みの重みをロードし、訓練モードに設定しました')","execution_count":22,"outputs":[{"output_type":"stream","text":"ネットワーク設定完了：学習済みの重みをロードし、訓練モードに設定しました\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 損失関数の設定\n#criterion = nn.CrossEntropyLoss()\ncriterion = nn.MSELoss()","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 最適化手法の設定\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# モデルを学習させる関数を作成\n\n\ndef train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n\n    # 初期設定\n    # GPUが使えるかを確認\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"使用デバイス：\", device)\n\n    # ネットワークをGPUへ\n    net.to(device)\n\n    # ネットワークがある程度固定であれば、高速化させる\n    torch.backends.cudnn.benchmark = True\n\n    # epochのループ\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-------------')\n\n        # epochごとの訓練と検証のループ\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                net.train()  # モデルを訓練モードに\n            else:\n                net.eval()   # モデルを検証モードに\n\n            epoch_loss = 0.0  # epochの損失和\n            epoch_corrects = 0  # epochの正解数\n\n            # 未学習時の検証性能を確かめるため、epoch=0の訓練は省略\n            if (epoch == 0) and (phase == 'train'):\n                continue\n\n            # データローダーからミニバッチを取り出すループ\n            for inputs, labels in dataloaders_dict[phase]:\n\n                # GPUが使えるならGPUにデータを送る\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n            \n                \n                #print(labels)\n                \n                  # optimizerを初期化\n                optimizer.zero_grad()\n\n                # 順伝搬（forward）計算\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = net(inputs)\n                    loss = criterion(outputs, labels)  # 損失を計算\n                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n\n                    # 訓練時はバックプロパゲーション\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                    # 結果の計算\n                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n                    # 正解数の合計を更新\n                    #epoch_corrects += torch.sum(preds == labels.data)\n\n            # epochごとのlossと正解率を表示\n            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n            #epoch_acc = epoch_corrects.double(\n            #) / len(dataloaders_dict[phase].dataset)\n\n            print('{} Loss: {:.4f}'.format(\n                phase, epoch_loss))\n            \n            print('{} Loss: {:.4f}'.format(\n                phase, epoch_loss))\n\n","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 学習・検証を実行する\nnum_epochs=20\ntrain_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n","execution_count":30,"outputs":[{"output_type":"stream","text":"使用デバイス： cuda:0\nEpoch 1/20\n-------------\n","name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"name 'epoch_acc' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-162e7003aec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 学習・検証を実行する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-bb61d8363046>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, dataloaders_dict, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n\u001b[0;32m---> 72\u001b[0;31m                 phase, epoch_loss, epoch_acc))\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             print('{} Loss: {:.4f}'.format(\n","\u001b[0;31mNameError\u001b[0m: name 'epoch_acc' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PyTorchのネットワークパラメータの保存\nsave_path = './ranzcr_vgg16_weights_fine_tuning.pth'\ntorch.save(net.state_dict(), save_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PyTorchのネットワークパラメータのロード\nload_path = './ranzcr_vgg16_weights_fine_tuning.pth'\nload_weights = torch.load(load_path)\nnet.load_state_dict(load_weights)\n\n# GPU上で保存された重みをCPU上でロードする場合\nload_weights = torch.load(load_path, map_location={'cuda:0': 'cpu'})\nnet.load_state_dict(load_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}